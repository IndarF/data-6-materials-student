{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a9db2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab13.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccea612",
   "metadata": {},
   "source": [
    "<img src=\"data6.png\" style=\"width: 15%; float: right; padding: 1%; margin-right: 2%;\"/>\n",
    "\n",
    "# Lab 13 – APIs, Prompt Engineering\n",
    "\n",
    "## Data 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from IPython.display import YouTubeVideo, HTML, display\n",
    "from ipywidgets import interact, widgets\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c240b",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 1: Cohen's Kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc19be",
   "metadata": {},
   "source": [
    "**Cohen’s Kappa** is a measure of agreement between two annotators (or raters) who independently classify items into categories.\n",
    "\n",
    "Unlike simple agreement rates (e.g., how often the two raters agree), **Cohen’s Kappa adjusts for chance agreement,** that is, how often two people might agree just by random guessing.\n",
    "\n",
    "Read the [Data 6 Notes](https://data6.org/notes/20-coding/cohens-kappa.html) for a full description of this measure, along with a walkthrough example.\n",
    "\n",
    "## Cohen’s Kappa Formula\n",
    "\n",
    "Cohen's kappa measures the agreement between two raters who each classify items into a set of mutually exclusive categories. Here is the mathematical notation of Cohen's Kappa:\n",
    "\n",
    "$$\\kappa = \\frac{p_o - p_e}{1 - p_e}$$\n",
    "\n",
    "Where:\n",
    "- $p_o$ = observed agreement rate, i.e., how often the raters agreed\n",
    "- $p_e$ = random agreement rate, i.e., how likely the raters would agree just by randomly guessing.\n",
    "\n",
    "We will not go into the probability in this course, but here's the idea. Cohen's Kappa is a **ratio** of two values:\n",
    "\n",
    "$$\\kappa = \\dfrac{\\text{observed agreement rate} - \\text{random agreement rate}}{1 - \\text{random agreement rate}}$$\n",
    "\n",
    "If the raters are in complete agreement, then the observed agreement rate $p_o = 1$ and $\\kappa = 1$. If the raters only agree randomly, then the observed agreement rate $p_o = p_e$ and $\\kappa = 0$. It is possible for $\\kappa < 0$, which can occur if there is really no relationship between the raters' rankings, or if raters are biased in their ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4fa7c",
   "metadata": {},
   "source": [
    "\n",
    "**In general, we will **not** ask you to manually compute Cohen's Kappa**. We will see that there is a convenient Python library called sklearn for computing Cohen's Kappa in practice. However, it is good to first internalize this idea of \"random chance\" (as we will see in the first question) with a manual computation before we rely on the library (as we will see in the second question).\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 1: Binary Classification\n",
    "\n",
    "\n",
    "In this question, we will manually compute Cohen's Kappa on a binary labeling task. This question just uses very simple Python (like a calculator!), but make sure you understand how we compute each part of the Cohen's Kappa formula. This example is sourced from the [Wikipedia article on Cohen's Kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa).\n",
    "\n",
    "Suppose that you were analyzing data related to a group of 50 people applying for a grant. Each grant proposal was read by two readers and each reader decides either \"Yes\" or \"No\" to the proposal. Suppose the summary of readers A and B's decisions were as follows:\n",
    "\n",
    "|             | B: Yes | B: No |\n",
    "| ---         | --- | --- |\n",
    "| <b>A: Yes</b> | 20 | 5 | \n",
    "| <b>A: No</b>  | 10 | 15 |\n",
    "\n",
    "\n",
    "This means:\n",
    "- Both A and B agreed on 35 grants:\n",
    "    - Both said **Yes** on 20 grants.\n",
    "    - Both said **No** on 15 grants.\n",
    "- A and B disagreed on 15 grants:\n",
    "    - A said **Yes**, B said **No** on 5 grants.\n",
    "    - A said **No**, B said **Yes** on 10 grants.\n",
    "\n",
    "Run the below cell to load the `grant_decisions` table with A and B's decision for each grant, one per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219371c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "grant_decisions = Table.read_table(\"grants.csv\")\n",
    "grant_decisions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e634b73c",
   "metadata": {},
   "source": [
    "We can verify that we can recreate the \"matrix\" above as a pivot table of `grant_decisions` with differently labeled columns and reordered rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad7b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "# verify the values make sense to you based on the matrix in the description above\n",
    "grant_decisions.pivot(\"B\", \"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff5ae6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 1a: Compute observed agreement rate, $p_o$\n",
    "\n",
    "The rate of observed agreement $p_o$ is the fraction of grants for which A and B actually agreed on their decision, i.e., they both decided \"yes\" or both decided \"no\". Use the `grant_decisions` table to compute this value and assign it to `p_o`.\n",
    "\n",
    "_Hints_:\n",
    "* We have used `num_yes_yes` and `num_no_no` to get the count of rows in `grant_decisions` for which A and B both decided \"yes\" and both decided \"no\", respectively. Our expression used method chaining. We then used these values to compute `p_o`, factoring in the total number of grants.\n",
    "* If you are still stuck, refer to the [Wikipedia article](https://en.wikipedia.org/wiki/Cohen%27s_kappa#Simple_example) which walks through the computation for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f60676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_yes_yes = ...\n",
    "num_no_no = ...\n",
    "p_o = ...\n",
    "p_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be63bc73",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc64e652",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 1b: Compute random agreement rate, $p_e$\n",
    "\n",
    "The rate of random agreement $p_e$ is the _hypothetical_ (i.e., expected) fraction of grants for which A and B might randomly agree. That is, if A had randomly voted yes on agreements _based on A's observed \"yes\" rate_, and B had randomly also voted yes on agreements _based on B's observed \"yes\" rate_, then sometimes A and B might agree in their decisions purely based on chance.\n",
    "\n",
    "Read the [Data 6 Notes](https://data6.org/notes/20-coding/cohens-kappa.html) for a suggested approach to computing $p_e$, rooted in probability theory (out of scope). Use the `grant_decisions` table to compute the random agreement rate and assign it to `p_e`.\n",
    "\n",
    "_Hints_:\n",
    "* We have defined `rate_yes_a` and `rate_yes_b` to get the \"yes\" rates of A and B (Step 1).\n",
    "* We then used these rates to compute `chance_yes_yes` and `chance_no_no` to compute probabilities in Steps 2 and 3.\n",
    "* Finally, we used these chance probabilities to compute the random agreement rate in Step 4.\n",
    "* If you are still stuck, refer to the [Wikipedia article](https://en.wikipedia.org/wiki/Cohen%27s_kappa#Simple_example) which walks through the computation for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488addb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rate_yes_a = ...\n",
    "rate_yes_b = ...\n",
    "chance_yes_yes = ...\n",
    "chance_no_no = ...\n",
    "p_e = ...\n",
    "p_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5572ed2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d29e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 1c: Compute Cohen's Kappa, $\\kappa$\n",
    "\n",
    "Use your values of $p_o$ and $p_e$ to compute Cohen's Kappa and assign it to `kappa_manual`. Here is the formula again:\n",
    "\n",
    "\n",
    "$$\\kappa = \\frac{p_o - p_e}{1 - p_e}$$\n",
    "\n",
    "Again, we have included the text description of this formula in case it is easier to work through:\n",
    "\n",
    "$$\\kappa = \\dfrac{\\text{observed agreement rate} - \\text{random agreement rate}}{1 - \\text{random agreement rate}}$$\n",
    "\n",
    "As per the [Wikipedia article](https://en.wikipedia.org/wiki/Cohen%27s_kappa#Simple_example), this value should be $0.4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaa3bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kappa_manual = ...\n",
    "kappa_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8caed89",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa33c0a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 1d: Determine Agreement Level\n",
    "\n",
    "Finally, use the Landis and Koch (1981, [DOI](https://doi.org/10.2307/2529310)) table to determine the level of agreement.\n",
    "\n",
    "| $\\kappa$ | Agreement |\n",
    "| --- | --- |\n",
    "| $ < 0$ | no agreement |\n",
    "| $ 0-0.2$ | poor |\n",
    "| $ 0.21-0.4 $ | fair | \n",
    "| $ 0.41-0.6 $ | moderate |\n",
    "| $ 0.61 - 0.8 $ | good |\n",
    "| $ 0.8 - 1.0 $ | near-perfect agreement |\n",
    "\n",
    "Assign `agreement` to one of the Agreement strings above based on your value of `kappa_manual`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8019f25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agreement = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3499d091",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22064051",
   "metadata": {},
   "source": [
    "As mentioned earlier, in practice we do not manually compute Cohen's Kappa. As you may have gathered from your work so far, manual computation is fraught with many potential missteps, because there are many intermediate rates and probabilities to compute.\n",
    "\n",
    "Instead, many instructors use the scikit-learn library, or **sklearn**, a Python package that is a toolkit for many scientific and machine learning applications. We will see that there is a much shorter way of reliably computing Cohen's Kappa. Onwards!\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 2: scikit-learn (sklearn)\n",
    "\n",
    "The Cohen's Kappa metric is provided in the `cohen_kappa_score` function.\n",
    "    \n",
    "Run the below cell to import the `cohen_kappa_score` from the `sklearn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b1fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85703e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2a\n",
    "\n",
    "Read about how to use this function in the [sklearn docs](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html). **Pay special attention to the argument types of `y1` and `y2`** (they are arrays).\n",
    "\n",
    "Then, in the cell below, provide the appropriate columns of `grant_decisions` to pass in as arguments to the `cohen_kappa_score` function call. After running your cell, `kappa_sk` should be the same (or close to the same) value as the `kappa_manual` that you computed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8938ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kappa_sk = cohen_kappa_score(..., ...)\n",
    "kappa_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce489be9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a5fe2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2b\n",
    "\n",
    "The `sklearn` function is flexible and can handle cases where the classification or labeling task involves more than just binary labels.\n",
    "\n",
    "Suppose we had a different group of 50 people applying for a grant, where each grant proposal was read by two readers. However, these two readers C and D can decide either \"Yes\", \"No\", or \"Abstain\" for each proposal. Note that now there are **three** categories!\n",
    "\n",
    "| | D: yes\t| D: no\t| D: abstain |\n",
    "| --- | --- | --- | --- |\n",
    "| <b>C: yes</b> |\t11 |\t12 |\t1 |\n",
    "| <b>C: no</b> |\t15 |\t5 |\t4 |\n",
    "| <b>C: abstain</b> |\t0 |\t2 |\t0 |\n",
    "\n",
    "Run the two cells below to load in the `grant_decisions_multi` table and reconstruct the matrix above, again with rows/columns potentially shuffled and relabeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d48a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "grant_decisions_multi = Table.read_table(\"grants_multi.csv\")\n",
    "grant_decisions_multi.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8384af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "# verify the values make sense to you based on the matrix in the description above\n",
    "grant_decisions_multi.pivot(\"D\", \"C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b7b019",
   "metadata": {},
   "source": [
    "It would be challenging to compute Cohen's Kappa in this case by hand. Instead, the `cohen_kappa_score` function from the sklearn library makes it easy.\n",
    "\n",
    "In the cell below, provide the appropriate columns of `grant_decisions_multi` to pass in as arguments to the `cohen_kappa_score` function call, and assign the Cohen's Kappa score to `kappa_multi`. Your approach should be structurally *very similar* to what you did in the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bd53ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kappa_multi = cohen_kappa_score(..., ...)\n",
    "kappa_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e162657",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8ff438",
   "metadata": {},
   "source": [
    "Note that in this case, we get a **negative** Cohen's Kappa score. This means that readers C and D reached very little agreement. We can see this in the matrix above, where there are more grants for which C and D disagree (one votes \"yes\", one votes \"no\") compared to those for which they agree (both vote \"yes\" or both vote \"no\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada7baa",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 2: The Google Gemini API\n",
    "\n",
    "This part gives you your first experience using the Gemini Python API. We walk through the official tutorials and try to abstract away the complicated parts of the official documentation.\n",
    "\n",
    "Resources:\n",
    "\n",
    "* [Gemini API Quickstart](https://ai.google.dev/gemini-api/docs/quickstart)\n",
    "* [Google Gen AI Python SDK](https://googleapis.github.io/python-genai/)\n",
    "* [Gemini API: Prompting Strategies](https://ai.google.dev/gemini-api/docs/prompting-strategies)\n",
    "* [Ziem et al., Table 1](https://direct.mit.edu/view-large/figure/4722326/coli_a_00502_i004.tif): LLM Prompting Guidelines to generate consistent, machine-readable outputs for CSS tasks. (very useful for project!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677a8933",
   "metadata": {},
   "source": [
    "\n",
    "## [Tutorial] Install Google/Gemini Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47869625",
   "metadata": {},
   "source": [
    "Google's Gemini API uses a Python interface. This is unlike the Genius and Wikipedia APIs, which used special URLs to access data.\n",
    "\n",
    "To access the Gemini API, we install the `google-genai` and related `google` packages. Run the cell below to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aa6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "!pip install google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab714ae",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Question 3: Gemini API Key\n",
    "\n",
    "We shared your API key with you through email. This API key allows Gemini to recognize who is using their API. In `api_key.py`, set `my_client_access_token` to be the string of alphanumeric characters that we provided you. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "_**Important Note**_:\n",
    "\n",
    " Please DO NOT share your API key outside of this class. We will disable your API key (1) if you misuse it and/or exceed the free usage tier during the project, and (2) for all students after the semester has ended. If you'd like to play around with your code after the term, you'll have to get your own API key.** Ask us how to do this!\n",
    "\n",
    "</div>\n",
    "\n",
    "Follow the corresponding instructions in the [Data 6 Notes](https://data6.org/notes/18-html/genius.html) to navigate to the `api_key.py` file and edit it.\n",
    "\n",
    "After you have updated `api_key.py` (the file within this assignment directory), running the below line should load your API token into `GOOGLE_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b90e69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# before running this cell, make sure that you have updated api_key.py with your API key\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import api_key\n",
    "GOOGLE_API_KEY = api_key.my_client_access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7afc2f8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6570b228",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# [Tutorial] Our first (two) API request(s)\n",
    "\n",
    "The APIs that we've worked with so far (Genius, Wikipedia) have been what is called [RESTful APIs](https://en.wikipedia.org/wiki/REST), where we specify a URL to access structured data. On the other hand, we are going to directly use a Python library API, developed by Google as part of their Gemini Software Development Kit (Gemini SDK).\n",
    "\n",
    "Using a Python API means that we will make a Python client object using Google's `genai` package, then call that object's methods to get data from Gemini. The Python API approach is often more convenient than the RESTful API approach when (1) the input and output are both quite flexible in format, as is the case for Generative AI data, and (2) when we need to make a lot of different calls to the API, perhaps within different functions.\n",
    "\n",
    "Run the cell below to import Google's `genai` Python module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2544378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677508c8",
   "metadata": {},
   "source": [
    "We will look at the example API request listed in the [Gemini Quickstart Documentation](https://ai.google.dev/gemini-api/docs/quickstart) (\"Make your first request\"), which you are welcome (and encouraged!) to browse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba39d9",
   "metadata": {},
   "source": [
    "### [Tutorial] From chat interface to API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8cb768",
   "metadata": {},
   "source": [
    "Consider the chat prompt shown in the screenshot, as well as (the start of) the model's response.\n",
    "\n",
    "<img src=\"prompt-chat.png\" alt=\"A screenshot of a Google Gemini chat prompt. Prompt is 'Explain how AI works in a few words.'. Response from Gemini chat is long but gets at the idea.\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d2890",
   "metadata": {},
   "source": [
    "An **AI chatbot** (like what is shown in the screenshot) is an application *on top of* a **large language model (LLM)**. The **LLM** is what takes in user prompts and returns text responses. The **chatbot** is what filters input, perhaps converting and loading files with additional prompts, and returns filtered LLM responses back, perhaps with some HTML or Markdown formatting.\n",
    "\n",
    "Let's break down what is happening in the above screenshot:\n",
    "\n",
    "* The user **prompt**: \"Explain how AI works in a few words.\"\n",
    "* The model **response**: \"AI works by using algorithms to analyze...\"\n",
    "* The specified **model**: Here, it is \"Fast\" (note the dropdown in the bottom right). The other option is \"Thinking.\" We'll discuss this more later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7b5c5",
   "metadata": {},
   "source": [
    "### [Tutorial] Create the client then make the request\n",
    "\n",
    "Run the next cell, which uses Gemini's Python API to directly query the Gemini LLM with this prompt. We describe the behavior below the cell, so just run the cell first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063c9455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "\n",
    "# first, create the client\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# then make the API request\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e7c78",
   "metadata": {},
   "source": [
    "What just happened (and how you would make these API calls):\n",
    "\n",
    "1. Make an **API client**: Make a Gemini client object called `client`, which passes in your API key. (We will do this for you usually.) This client lets you access different API calls via its methods. We will focus on one specific method in this class, `generate_content`.\n",
    "1. Make the **API request**. Call `generate_content`, available in via `client.models`. This cell takes in several arguments, which we specify by name:\n",
    "\n",
    "    * `model`: The underlying large language model. Here we specify Gemini 2.5 Flash, which is equivalent to the \"Fast\" option in the Gemini chatbot.\n",
    "    * `contents`: The **prompt**: \"Explain how AI works in a few words.\" We discuss the type of `contents` later.\n",
    "1. Receive a **response**. `response` is a [Gemini-specific object type](https://ai.google.dev/gemini-api/docs/quickstart). The details are too complicated for this course. Instead, we focus on the value **`response.text`**, the LLM response string itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6675a5cd",
   "metadata": {},
   "source": [
    "### [Tutorial] Make another API request\n",
    "\n",
    "Once we have created the API client, we do not need to remake the client. Instead, we can just make more calls to `generate_content`.\n",
    "\n",
    "Run the example cell below to run the same prompt. Remember—**LLMs are random response generators**, so you will likely get a different response back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bfa0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "\n",
    "# client is already created, so just call generate_content\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05917a19",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 4 – Understand the API Request\n",
    "\n",
    "Let us examine the structure of this API request, which is effectively just a fancy function call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c97433",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 4a\n",
    "\n",
    "In the `client.models.generate_content` function call above, what is the data type of the argument passed to `model`? Assign `q4a` to an integer corresponding to the appropriate choice below.\n",
    "\n",
    "1. Integer\n",
    "2. Response\n",
    "3. String\n",
    "4. Array\n",
    "5. List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2999f843",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q4a = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9ce413",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560b2b53",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 4b\n",
    "\n",
    "Consider the response to the API request. In the example code, what is the data type of `response.text`? Assign `q4b` to an integer corresponding to the appropriate choice below.\n",
    "\n",
    "1. Integer\n",
    "2. Response\n",
    "3. String\n",
    "4. Array\n",
    "5. List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3bb97b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q4b = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70233948",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5567f3ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 4c\n",
    "\n",
    "In the `client.models.generate_content` API call above, what is the data type of the argument passed to `contents`? Assign `q4c` to an integer corresponding to the appropriate choice below.\n",
    "\n",
    "1. Integer\n",
    "2. Response\n",
    "3. String\n",
    "4. Array\n",
    "5. List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca644030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q4c = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b181b6f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f9f91",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 5 – Your Turn\n",
    "\n",
    "In the cell below, make an API request to see what the best restaurant in Berkeley is.\n",
    "\n",
    "Make this request by calling `client.models.generate_content`. We recommend copying the example prompt above, then modifying `contents` to fit your needs. Finally, assign `restaurant_str` to the response string (done for you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78f22a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = ...\n",
    "\n",
    "restaurant_str = response.text # we have done this for you\n",
    "print(restaurant_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635bc4b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d25e343",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 6 – Different Gemini Models\n",
    "\n",
    "You can specify different Gemini models to get different responses. Some more advanced models will produce higher-quality responses, though with the tradeoff that these responses take longer to get.\n",
    "\n",
    "We recommend two models for this course, which are relatively fast and inexpensive:\n",
    " * \"Fast\" **Gemini 2.5 Flash** model (`\"gemini-2.5-flash\"`). Pretty fast, though depending on the time of day responses might still take a few seconds.\n",
    " * \"Thinking\" **Gemini 2.5 Pro** model (`\"gemini-2.5-pro\"`). More in-depth responses, though responses will take significantly longer than Fast responses.\n",
    "\n",
    "In the cell below, paste your prompt from earlier and modify the model to use Gemini 2.5 Pro. Assign `restaurant_str_pro` to the response string (done for you). Note that it may take up to a minute or two to get a response back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5cfd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model= ...\n",
    "    contents= ...\n",
    ") \n",
    "\n",
    "restaurant_str_pro = response.text # we have done this for you\n",
    "print(restaurant_str_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631b95a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d908554e",
   "metadata": {},
   "source": [
    "**Discuss** (no response needed): Compare the responses from Gemini 2.5 Flash and Gemini 2.5 Pro. Are there perceivable differences in length? quality? tone? format? credibility?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ae0b2",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "_**Important Note**_:\n",
    "\n",
    "Gemini Pro not only has longer request times; it is also more expensive per request. We **strongly** recommended testing out prompts with **Gemini 2.5 Flash**. Once you are satisfied with your prompt, only then should you change models. (Otherwise you'll be stuck waiting for a while in-between requests...)\n",
    "\n",
    "</div>\n",
    "\n",
    "We use **Gemini 2.5 Flash** for the remainder of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56da5421",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# [Tutorial] Providing more context for prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d0918c",
   "metadata": {},
   "source": [
    "From [Wikipedia](https://en.wikipedia.org/wiki/Prompt_engineering): **Prompt engineering** is the process of structuring or crafting a **prompt** (natural language text instruction) in order to produce better outputs from a generative artificial intelligence (AI) model.\n",
    "\n",
    "Depending on how you wrote your prompts above, you may find that the LLM responses were too general. For example, the top recommendations might not be ideal for college students (Chez Panisse is pretty pricey for a weeknight dinner...). A common method in prompt engineering a better response is to provide more **context**. Context can help define:\n",
    "\n",
    "* What information you are looking for in the response (\"Only include cheap meals under $15, and only consider restaurants that are open past 9pm.\")\n",
    "* How long you want the response to be (\"Limit your response to 200 words.\")\n",
    "* What tone you want the response to have (\"Imagine you are a UC Berkeley student talking to a fellow classmate.\")\n",
    "* How to structure the response: markdown, comma-separated, JSON, etc. (\"Format your response as a bulleted list.\")\n",
    "* Include linebreaks (with newline characters `'\\n'`, or use multi-line strings) to delineate different aspects of the prompt\n",
    "\n",
    "One way to provide detailed context is to just pass in a very, very long string as your prompt.\n",
    "\n",
    "Run the cell below for an example. Note that the triple quotes (`\"\"\"`) allows you to specify a multi-line string, with line breaks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08bbc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"\"\"Imagine you are a UC Berkeley student talking to a fellow classmate.\n",
    "    \n",
    "    What is the best restaurant in Berkeley? Only include cheap meals under $15,\n",
    "    and only consider restaurants that are open past 9pm.\n",
    "\n",
    "    Format your response as a bulleted list. Limit your response to 200 words.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419dd70",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "As we've seen in this class time and time again, we want to **reuse** pieces of code, data, etc. The same is true of our contexts! One formatting context may be very useful for an entirely different application, and it would be great to use those same strings.\n",
    "\n",
    "The Gemini API supports multiple argument types for `content`. For this course, we focus on providing additional **context** to our prompts by passing in a **list of strings** to `content`. This allows us to reuse context parts. It is also comparable to separating context parts with linebreaks.\n",
    "\n",
    "Run the cell below, which specifies the same prompt as before but now with a list of strings passed into `content`. (Again, because LLMs are random text generators, the response may be different from before.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0f1683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "context_character = \"Imagine you are a UC Berkeley student talking to a fellow classmate.\"\n",
    "context_format = \"Format your response as a bulleted list. Limit your response to 200 words.\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=[\n",
    "        context_character,\n",
    "        \"\"\"\n",
    "        What is the best restaurant in Berkeley? Only include cheap meals under $15,\n",
    "        and only consider restaurants that are open past 9pm.    \n",
    "        \"\"\",\n",
    "        context_format\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252bdbf0",
   "metadata": {},
   "source": [
    "## Prompt Engineering Resources\n",
    "\n",
    "There are many guides out there to good prompt engineering. Here are a few we recommend:\n",
    "\n",
    "* [Gemini API: Prompting Strategies](https://ai.google.dev/gemini-api/docs/prompting-strategies)\n",
    "* [Ziem et al., Table 1](https://direct.mit.edu/view-large/figure/4722326/coli_a_00502_i004.tif): LLM Prompting Guidelines to generate consistent, machine-readable outputs for CSS tasks. (very useful for project!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a9a339",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 7 – Your Turn\n",
    "\n",
    "Now, it's your turn. Let's review functions and conditionals together. Consider the [Data 6 Fall 2025 Quiz 3](https://data6.org/fa25/exams/quizzes/fa25-quiz3.pdf) `hot_or_not` question on page 4. We'd like to write one new questions identical in structure to Q2.1-2.2 but with a different argument to the function call.\n",
    "\n",
    "Using the Prompt Engineering Resources above, write an API request that returns these two new questions. Try to prevent the LLM from giving you the answer—it should just give the same four multiple choice options!\n",
    "\n",
    "Your updated prompt should also do the following:\n",
    "1. Include a brief context (15 words or less) \n",
    "2. Enumerate options as alphabetical multiple choice, separated by a new line.\n",
    "3. Specify constraints\n",
    "4. Specify actions under uncertainty\n",
    "5. Provide at least 1 example of expected input and output\n",
    "\n",
    "You should use Gemini 2.5 Flash, not Pro. Assign `question_str` to the response string (done for you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0831fe2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context_hot_or_not = \"\"\"def hot_or_not(rank):\n",
    "    if rank < 50:\n",
    "        return \"hot\"\n",
    "    elif rank > 50:\n",
    "        return \"not bad\"\n",
    "\"\"\" # provided for you\n",
    "context_example = ... \n",
    "response = ...\n",
    "question_str = response.text # we have done this for you\n",
    "print(question_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e403300",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 3: Final Project Planning\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 8\n",
    "\n",
    "Refer to [the announcement in Lecture 20](https://docs.google.com/presentation/d/1-ljp0mvgDFZYQgh5gkuSMl6oYSKxHZUwiB-nGoG4EPs/edit?slide=id.g3a676c0168b_6_11#slide=id.g3a676c0168b_6_11), which outlines the final project.\n",
    "\n",
    "The Final Project is partnered. **Fill out the below form as part of lab credit this week.**\n",
    "\n",
    "> [Data 6 Fall 2025 Final Project Partner Matching Form](https://docs.google.com/forms/d/e/1FAIpQLSdyhmG1bms5yq9rY8dAi4RYhybJua9esN8NoZ76ugOan3f1Uw/viewform?usp=header)\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "* If you have a project partner in mind (even if from a different section), both partners must fill out the form and specify each other as partner.\n",
    "* If you do not have a partner in mind, fill out the form to indicate your project partner preferences and we will match you, prioritizing similar students who are in the same section.\n",
    "* If you do not fill out the form, we will randomly match you, prioritizing students who are in the same section.\n",
    "\n",
    "There is nothing else to do for this question beyond filling out the Google Form."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d338a5c",
   "metadata": {},
   "source": [
    "## Pets of Data 6\n",
    "\n",
    "Congratulations on completing Lab 13!\n",
    "\n",
    "<img src=\"portobello_caramello.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59098b8d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b22d219",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f0ff9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50666c9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a93a82",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 0 < p_o < 1\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> p_o == 0.7\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 0 < p_e < 1\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> p_e == 0.5\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> -1 < kappa_manual < 1\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(kappa_manual, 0.4)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1d": {
     "name": "q1d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(agreement, str)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> agreement in [\"no agreement\", \"poor\", \"fair\", \"moderate\", \"good\", \"near-perfect agreement\"]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> agreement == \"fair\"\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> -1 < kappa_sk < 1\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(kappa_sk, 0.4)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(kappa_sk, kappa_manual)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> -1 < kappa_multi < 1\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> kappa_multi < 0\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(np.round(kappa_multi, 4), -0.2057)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(GOOGLE_API_KEY) == str\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(q4a, int)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q4a == 3\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(q4b, int)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q4b == 3\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4c": {
     "name": "q4c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(q4c, int)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q4c == 3\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(restaurant_str, str)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(restaurant_str_pro, str)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
